volumes:
  ollama-models:
  redis-data:
  node-modules:
  searxng-cache:

services:
  redis:
    image: redis:7-alpine
    container_name: kestrel-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    networks:
      - kestrel-network

  backend:
    build:
      dockerfile: ./Dockerfiles/backend_dockerfile
    image: ghcr.io/dankeg/kestrelai-backend:commit-${COMMIT_SHA}
    container_name: kestrel-backend
    restart: unless-stopped
    working_dir: /app
    ports:
      - "8000:8000"
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - REDIS_URL=redis://redis:6379
      - OLLAMA_BASE_URL=http://ollama:11434
      - SEARXNG_URL=http://searxng:8080
    volumes:
      - ./KestrelAI:/app
      - ./notes:/app/notes
      - ./exports:/app/exports
    command: uvicorn backend.main:app --host 0.0.0.0 --port 8000 --reload
    depends_on:
      redis: { condition: service_started }
      ollama: { condition: service_started }
      ollama-init: { condition: service_completed_successfully }
    networks:
      - kestrel-network

  frontend:
    build:
      dockerfile: ./Dockerfiles/frontend_dockerfile
    image: ghcr.io/dankeg/kestrelai-frontend:commit-${COMMIT_SHA}
    container_name: kestrel-frontend
    restart: unless-stopped
    working_dir: /app
    ports:
      - "5173:5173"
    environment:
      - REACT_APP_API_URL=http://localhost:8000/api/v1
      - NODE_ENV=development
    volumes:
      - ./kestrel-ui:/app
      - node-modules:/app/node_modules
    command: npm run dev -- --host 0.0.0.0
    depends_on:
      - backend
    networks:
      - kestrel-network

  agent:
    build:
      dockerfile: ./Dockerfiles/agent_dockerfile
    image: ghcr.io/dankeg/kestrelai-agent:commit-${COMMIT_SHA}
    container_name: kestrel-agent
    restart: unless-stopped
    working_dir: /app
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - SEARXNG_URL=http://searxng:8080
      - MODEL_NAME=${MODEL_NAME:-llama3.1}
      - OLLAMA_KEEP_ALIVE=24h
    volumes:
      - ./KestrelAI:/app
      - ./notes:/app/notes
    command: python -m model_loop
    depends_on:
      redis: { condition: service_started }
      searxng: { condition: service_started }
    networks:
      - kestrel-network

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_MODELS=/root/.ollama
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=1
    volumes:
      - ollama-models:/root/.ollama
    command: ["serve"]
    networks:
      - kestrel-network

  ollama-init:
    image: ollama/ollama:latest
    depends_on:
      ollama: { condition: service_started }
    environment:
      - OLLAMA_HOST=ollama:11434
      - MODEL_NAME=${MODEL_NAME:-llama3.1}
    volumes:
      - ollama-models:/root/.ollama
    entrypoint: ["/bin/sh","-lc"]
    command:
      - >
        until ollama list >/dev/null 2>&1; do sleep 1; done;
        ollama pull ${MODEL_NAME} || true
    restart: "no"
    networks:
      - kestrel-network

  searxng:
    image: searxng/searxng:latest
    container_name: searxng
    restart: unless-stopped
    ports:
      - "8080:8080"
    environment:
      - SEARXNG_BASE_URL=http://localhost:8080/
      - SEARXNG_SETTINGS_PATH=/etc/searxng/settings.yml
      - SEARXNG_REDIS_URL=redis://redis:6379/0
      - BIND_ADDRESS=0.0.0.0:8080
      - FORCE_OWNERSHIP=true
    volumes:
      - ./searxng:/etc/searxng
      - searxng-cache:/var/cache/searxng
    depends_on:
      redis: { condition: service_started }
    networks:
      - kestrel-network

networks:
  kestrel-network:
    driver: bridge
